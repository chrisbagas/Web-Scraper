{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9148b369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Custom retry logger\n",
    "def log_request_retry(method, url, retries_left):\n",
    "    print(f\"[Retry] {method} {url} | Retries left: {retries_left}\")\n",
    "\n",
    "# Custom HTTPAdapter with verbose retry logging\n",
    "class VerboseHTTPAdapter(HTTPAdapter):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.max_retries_config = kwargs.get(\"max_retries\")\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def send(self, request, **kwargs):\n",
    "        try:\n",
    "            return super().send(request, **kwargs)\n",
    "        except Exception as e:\n",
    "            log_request_retry(request.method, request.url, self.max_retries_config.total)\n",
    "            raise\n",
    "\n",
    "# Create session with retries and mount custom adapter\n",
    "retry_strategy = Retry(\n",
    "    total=5,\n",
    "    status_forcelist=[429, 500, 502, 503, 504],\n",
    "    backoff_factor=1,\n",
    "    allowed_methods=[\"GET\", \"POST\"]\n",
    ")\n",
    "session = requests.Session()\n",
    "adapter = VerboseHTTPAdapter(max_retries=retry_strategy)\n",
    "session.mount(\"https://\", adapter)\n",
    "session.mount(\"http://\", adapter)\n",
    "\n",
    "# Scraping function using session\n",
    "def scrape_guardian_graphql(product_keywords):\n",
    "    url = \"https://guardianindonesia.co.id/graphql\"\n",
    "    site_url = \"https://guardianindonesia.co.id\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "\n",
    "    def build_payload(keyword, page):\n",
    "        return {\n",
    "            \"operationName\": \"ProductSearch\",\n",
    "            \"query\": \"\"\"\n",
    "                query ProductSearch($currentPage:Int=1, $inputText:String!, $pageSize:Int=6, $filters:ProductAttributeFilterInput!, $sort:ProductAttributeSortInput) {\n",
    "                  products(currentPage:$currentPage, pageSize:$pageSize, search:$inputText, filter:$filters, sort:$sort) {\n",
    "                    items {\n",
    "                      name\n",
    "                      url_key\n",
    "                      stock_status\n",
    "                      price_range {\n",
    "                        minimum_price {\n",
    "                          final_price { value }\n",
    "                          regular_price { value }\n",
    "                          discount { percent_off }\n",
    "                        }\n",
    "                      }\n",
    "                      promo\n",
    "                      small_image { url }\n",
    "                    }\n",
    "                    page_info {\n",
    "                      current_page\n",
    "                      total_pages\n",
    "                    }\n",
    "                  }\n",
    "                }\n",
    "            \"\"\",\n",
    "            \"variables\": {\n",
    "                \"currentPage\": page,\n",
    "                \"pageSize\": 12,\n",
    "                \"filters\": {},\n",
    "                \"inputText\": keyword,\n",
    "                \"sort\": {\"relevance\": \"DESC\", \"stock_status\": \"DESC\"}\n",
    "            }\n",
    "        }\n",
    "\n",
    "    for keyword in product_keywords:\n",
    "        print(f\"🔍 Searching for '{keyword}'...\")\n",
    "        page = 1\n",
    "        while True:\n",
    "            payload = build_payload(keyword, page)\n",
    "            print(f\"  → Requesting page {page}\")\n",
    "\n",
    "            try:\n",
    "                response = session.post(url, headers=headers, json=payload)\n",
    "            except Exception as e:\n",
    "                print(f\"  ✖ Request error on page {page}: {e}\")\n",
    "                break\n",
    "\n",
    "            if response.status_code != 200:\n",
    "                print(f\"  ✖ Failed to get data (status {response.status_code}) for {keyword} page {page}\")\n",
    "                break\n",
    "\n",
    "            data = response.json()\n",
    "            product_data = data.get(\"data\", {}).get(\"products\", {})\n",
    "            items = product_data.get(\"items\", [])\n",
    "            page_info = product_data.get(\"page_info\", {})\n",
    "            total_pages = page_info.get(\"total_pages\", 1)\n",
    "\n",
    "            if not items:\n",
    "                print(f\"  ✱ No items found on page {page}\")\n",
    "                break\n",
    "\n",
    "            for item in items:\n",
    "                try:\n",
    "                    name = item.get(\"name\", \"\")\n",
    "                    final_price = item[\"price_range\"][\"minimum_price\"][\"final_price\"][\"value\"]\n",
    "                    regular_price = item[\"price_range\"][\"minimum_price\"][\"regular_price\"][\"value\"]\n",
    "                    discount = item[\"price_range\"][\"minimum_price\"][\"discount\"].get(\"percent_off\", 0)\n",
    "                    promo = item.get(\"promo\") or \"-\"\n",
    "                    stock_status = item.get(\"stock_status\", \"\")\n",
    "                    url_key = item.get(\"url_key\", \"\")\n",
    "                    product_url = f\"{site_url}/{url_key}.html\"\n",
    "                    image_url = item.get(\"small_image\", {}).get(\"url\", \"\")\n",
    "\n",
    "                    results.append({\n",
    "                        \"Keyword\": keyword,\n",
    "                        \"Name\": name,\n",
    "                        \"Price\": final_price,\n",
    "                        \"Regular Price\": regular_price,\n",
    "                        \"Discount (%)\": discount,\n",
    "                        \"Promo\": promo,\n",
    "                        \"Stock Status\": stock_status,\n",
    "                        \"Product URL\": product_url,\n",
    "                        \"Image URL\": image_url\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"    ⚠ Error parsing item: {e}\")\n",
    "\n",
    "            if page >= total_pages:\n",
    "                break\n",
    "\n",
    "            page += 1\n",
    "            time.sleep(1)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27373602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Searching for 'clear'...\n",
      "  → Requesting page 1\n",
      "  → Requesting page 2\n",
      "  → Requesting page 3\n",
      "  → Requesting page 4\n",
      "  → Requesting page 5\n",
      "  → Requesting page 6\n",
      "  → Requesting page 7\n",
      "  → Requesting page 8\n",
      "  → Requesting page 9\n",
      "  → Requesting page 10\n",
      "  → Requesting page 11\n",
      "  → Requesting page 12\n",
      "  → Requesting page 13\n",
      "  → Requesting page 14\n",
      "  → Requesting page 15\n",
      "  → Requesting page 16\n",
      "  → Requesting page 17\n",
      "  → Requesting page 18\n",
      "  → Requesting page 19\n",
      "🔍 Searching for 'sunsilk'...\n",
      "  → Requesting page 1\n",
      "  → Requesting page 2\n",
      "🔍 Searching for 'lifebuoy'...\n",
      "  → Requesting page 1\n",
      "  → Requesting page 2\n",
      "  → Requesting page 3\n",
      "  → Requesting page 4\n",
      "🔍 Searching for 'tresemme'...\n",
      "  → Requesting page 1\n",
      "  → Requesting page 2\n",
      "  → Requesting page 3\n",
      "🔍 Searching for 'ponds'...\n",
      "  → Requesting page 1\n",
      "  → Requesting page 2\n",
      "  → Requesting page 3\n",
      "  → Requesting page 4\n",
      "  → Requesting page 5\n",
      "  → Requesting page 6\n",
      "  → Requesting page 7\n",
      "  → Requesting page 8\n",
      "  → Requesting page 9\n",
      "🔍 Searching for 'glow'...\n",
      "  → Requesting page 1\n",
      "  → Requesting page 2\n",
      "  → Requesting page 3\n",
      "  → Requesting page 4\n",
      "  → Requesting page 5\n",
      "  → Requesting page 6\n",
      "  → Requesting page 7\n",
      "  → Requesting page 8\n",
      "  → Requesting page 9\n",
      "  → Requesting page 10\n",
      "  → Requesting page 11\n",
      "  → Requesting page 12\n",
      "  → Requesting page 13\n",
      "  → Requesting page 14\n",
      "  → Requesting page 15\n",
      "  → Requesting page 16\n",
      "  → Requesting page 17\n",
      "  → Requesting page 18\n",
      "  → Requesting page 19\n",
      "  → Requesting page 20\n",
      "  → Requesting page 21\n",
      "  → Requesting page 22\n",
      "  → Requesting page 23\n",
      "  → Requesting page 24\n",
      "  → Requesting page 25\n",
      "  → Requesting page 26\n",
      "  → Requesting page 27\n",
      "  → Requesting page 28\n",
      "  → Requesting page 29\n",
      "  → Requesting page 30\n",
      "  → Requesting page 31\n",
      "  → Requesting page 32\n",
      "  → Requesting page 33\n",
      "  → Requesting page 34\n",
      "  → Requesting page 35\n",
      "  → Requesting page 36\n",
      "  → Requesting page 37\n",
      "  → Requesting page 38\n",
      "  → Requesting page 39\n",
      "  → Requesting page 40\n",
      "  → Requesting page 41\n",
      "  → Requesting page 42\n",
      "  → Requesting page 43\n",
      "  → Requesting page 44\n",
      "  → Requesting page 45\n",
      "  → Requesting page 46\n",
      "  → Requesting page 47\n",
      "  → Requesting page 48\n",
      "  → Requesting page 49\n",
      "  → Requesting page 50\n",
      "  → Requesting page 51\n",
      "  → Requesting page 52\n",
      "  → Requesting page 53\n",
      "  → Requesting page 54\n",
      "  → Requesting page 55\n",
      "  → Requesting page 56\n",
      "  → Requesting page 57\n",
      "  → Requesting page 58\n",
      "  → Requesting page 59\n",
      "  → Requesting page 60\n",
      "  → Requesting page 61\n",
      "  → Requesting page 62\n",
      "  → Requesting page 63\n",
      "🔍 Searching for 'vaseline'...\n",
      "  → Requesting page 1\n",
      "  → Requesting page 2\n",
      "  → Requesting page 3\n",
      "  → Requesting page 4\n",
      "  → Requesting page 5\n",
      "  → Requesting page 6\n",
      "🔍 Searching for 'pepsodent'...\n",
      "  → Requesting page 1\n",
      "  → Requesting page 2\n",
      "  → Requesting page 3\n",
      "  → Requesting page 4\n",
      "  → Requesting page 5\n",
      "🔍 Searching for 'dove'...\n",
      "  → Requesting page 1\n",
      "  → Requesting page 2\n",
      "  → Requesting page 3\n",
      "  → Requesting page 4\n",
      "  → Requesting page 5\n",
      "  → Requesting page 6\n",
      "🔍 Searching for 'colgate'...\n",
      "  → Requesting page 1\n",
      "  → Requesting page 2\n",
      "  → Requesting page 3\n",
      "  → Requesting page 4\n",
      "  → Requesting page 5\n",
      "🔍 Searching for 'closeup'...\n",
      "  → Requesting page 1\n",
      "🔍 Searching for 'lux'...\n",
      "  → Requesting page 1\n",
      "  → Requesting page 2\n",
      "  → Requesting page 3\n",
      "  → Requesting page 4\n",
      "  → Requesting page 5\n",
      "  → Requesting page 6\n",
      "  → Requesting page 7\n",
      "  → Requesting page 8\n",
      "  → Requesting page 9\n",
      "  → Requesting page 10\n",
      "  → Requesting page 11\n",
      "  → Requesting page 12\n",
      "  → Requesting page 13\n",
      "  → Requesting page 14\n",
      "  → Requesting page 15\n",
      "  → Requesting page 16\n",
      "  → Requesting page 17\n",
      "🔍 Searching for 'rexona'...\n",
      "  → Requesting page 1\n",
      "  → Requesting page 2\n",
      "  → Requesting page 3\n",
      "  → Requesting page 4\n",
      "🔍 Searching for 'axe'...\n",
      "  → Requesting page 1\n",
      "  → Requesting page 2\n",
      "🔍 Searching for 'head'...\n",
      "  → Requesting page 1\n",
      "  → Requesting page 2\n",
      "  → Requesting page 3\n",
      "  → Requesting page 4\n",
      "  → Requesting page 5\n",
      "  → Requesting page 6\n",
      "  → Requesting page 7\n",
      "🔍 Searching for 'pantene'...\n",
      "  → Requesting page 1\n",
      "  → Requesting page 2\n",
      "  → Requesting page 3\n",
      "  → Requesting page 4\n",
      "  → Requesting page 5\n",
      "  → Requesting page 6\n",
      "🔍 Searching for 'zinc'...\n",
      "  → Requesting page 1\n",
      "  → Requesting page 2\n",
      "  → Requesting page 3\n",
      "  → Requesting page 4\n",
      "  → Requesting page 5\n",
      "  → Requesting page 6\n",
      "  → Requesting page 7\n",
      "  → Requesting page 8\n",
      "  → Requesting page 9\n",
      "  → Requesting page 10\n",
      "  → Requesting page 11\n",
      "  → Requesting page 12\n",
      "  → Requesting page 13\n",
      "  → Requesting page 14\n",
      "  → Requesting page 15\n",
      "  → Requesting page 16\n",
      "  → Requesting page 17\n",
      "  → Requesting page 18\n",
      "  → Requesting page 19\n",
      "  → Requesting page 20\n",
      "  → Requesting page 21\n",
      "  → Requesting page 22\n",
      "  → Requesting page 23\n",
      "  → Requesting page 24\n",
      "  → Requesting page 25\n",
      "  → Requesting page 26\n",
      "  → Requesting page 27\n",
      "  → Requesting page 28\n",
      "  → Requesting page 29\n",
      "  → Requesting page 30\n",
      "  → Requesting page 31\n",
      "  → Requesting page 32\n",
      "  → Requesting page 33\n",
      "  → Requesting page 34\n",
      "  → Requesting page 35\n",
      "  → Requesting page 36\n",
      "  → Requesting page 37\n",
      "  → Requesting page 38\n",
      "🔍 Searching for 'garnier'...\n",
      "  → Requesting page 1\n",
      "  → Requesting page 2\n",
      "  → Requesting page 3\n",
      "  → Requesting page 4\n",
      "  → Requesting page 5\n",
      "  → Requesting page 6\n",
      "  → Requesting page 7\n",
      "  → Requesting page 8\n",
      "  → Requesting page 9\n",
      "  → Requesting page 10\n",
      "  → Requesting page 11\n",
      "🔍 Searching for 'nivea'...\n",
      "  → Requesting page 1\n",
      "  → Requesting page 2\n",
      "  → Requesting page 3\n",
      "  → Requesting page 4\n",
      "  → Requesting page 5\n",
      "  → Requesting page 6\n",
      "  → Requesting page 7\n",
      "  → Requesting page 8\n",
      "  → Requesting page 9\n",
      "  → Requesting page 10\n",
      "  → Requesting page 11\n",
      "🔍 Searching for 'marina'...\n",
      "  → Requesting page 1\n",
      "  → Requesting page 2\n",
      "  → Requesting page 3\n",
      "  → Requesting page 4\n",
      "🔍 Searching for 'ciptadent'...\n",
      "  → Requesting page 1\n",
      "  → Requesting page 2\n",
      "🔍 Searching for 'nuvo'...\n",
      "  → Requesting page 1\n",
      "  → Requesting page 2\n",
      "🔍 Searching for 'giv'...\n",
      "  → Requesting page 1\n",
      "  → Requesting page 2\n",
      "🔍 Searching for 'posh'...\n",
      "  → Requesting page 1\n",
      "  → Requesting page 2\n",
      "  → Requesting page 3\n",
      "  → Requesting page 4\n",
      "  → Requesting page 5\n",
      "🔍 Searching for 'citra'...\n",
      "  → Requesting page 1\n",
      "  → Requesting page 2\n",
      "🔍 Searching for 'rejoice'...\n",
      "  → Requesting page 1\n",
      "  → Requesting page 2\n"
     ]
    }
   ],
   "source": [
    "# 🔍 Example product keywords\n",
    "targets = ['clear', 'sunsilk', 'lifebuoy', 'tresemme', 'ponds',\n",
    "           'glow', 'vaseline', 'pepsodent','dove', 'colgate',\n",
    "           'closeup', 'lux', 'rexona', 'axe',\n",
    "           'head', 'pantene', 'zinc', 'garnier', 'nivea',\n",
    "           'marina', 'ciptadent', 'nuvo', 'giv', 'posh','citra', 'rejoice']\n",
    "data = scrape_guardian_graphql(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ad045c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done. Saved to 'guardian_products_graphql.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# 💾 Save to Excel\n",
    "df = pd.DataFrame(data)\n",
    "file_name = f\"../../guardian/GUARDIAN_{datetime.now().strftime('%y%m%d')}.xlsx\"\n",
    "df.to_excel(file_name, index=False)\n",
    "print(\"✅ Done. Saved to 'guardian_products_graphql.xlsx'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dc5ad0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
